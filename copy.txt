def chunk_text(text, chunk_size=500, overlap=50):
    """
    Split text into overlapping chunks.
    
    Parameters:
        text (str): Input text.
        chunk_size (int): Number of words per chunk.
        overlap (int): Number of overlapping words.
        
    Returns:
        list: List of text chunks.
    """
    words = text.split()
    chunks = []
    start = 0
    while start < len(words):
        end = start + chunk_size
        chunk = words[start:end]
        if not chunk:
            break
        chunks.append(" ".join(chunk))
        start += (chunk_size - overlap)
    return chunks
# med_discover_ai/config.py
import os
import torch

# --- Core Settings ---
# Determine if GPU is available for MedCPT models.
USE_GPU = torch.cuda.is_available()

# --- Model Configuration ---
if USE_GPU:
    print('GPU is available. Using MedCPT models.')
    # GPU mode: use MedCPT models.
    ARTICLE_ENCODER_MODEL = "ncbi/MedCPT-Article-Encoder"
    QUERY_ENCODER_MODEL = "ncbi/MedCPT-Query-Encoder"
    CROSS_ENCODER_MODEL = "ncbi/MedCPT-Cross-Encoder"
    EMBEDDING_MODEL = None # Not used in GPU mode for primary embedding
    EMBEDDING_DIMENSION = 768 # MedCPT embedding dimension
else:
    print('GPU not available. Using OpenAI embeddings (CPU mode).')
    # CPU mode: use OpenAI's embedding model.
    ARTICLE_ENCODER_MODEL = None
    QUERY_ENCODER_MODEL = None
    CROSS_ENCODER_MODEL = None
    EMBEDDING_MODEL = "text-embedding-ada-002" # Default OpenAI model
    EMBEDDING_DIMENSION = 1536 # Dimension for text-embedding-ada-002

# --- Text Processing Parameters ---
CHUNK_SIZE = 500 # Number of words per chunk
OVERLAP = 50     # Number of overlapping words between chunks
MAX_ARTICLE_LENGTH = 512 # Max tokens for article/cross encoder input
MAX_QUERY_LENGTH = 64     # Max tokens for query encoder input

# --- File Paths ---
DEFAULT_PDF_FOLDER = "./sample_pdf_rag" # Default location for input PDFs if needed
INDEX_SAVE_PATH = "./faiss_index.bin" # Path to save/load the FAISS index
DOC_META_PATH = "./doc_metadata.json" # Path to save/load document metadata

# --- OpenAI API Configuration ---
# Load API key from environment variable first, then fallback to a placeholder.
# IMPORTANT: It's best practice to set the API key via environment variables.
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY", "YOUR_OPENAI_API_KEY_HERE")
if OPENAI_API_KEY and OPENAI_API_KEY != "YOUR_OPENAI_API_KEY_HERE":
    os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY
    print("OpenAI API Key found.")
else:
    print("Warning: OpenAI API Key not found in environment variables. Please set it in the UI.")

# --- LLM Configuration ---
# List of available LLM models for the UI dropdown
AVAILABLE_LLM_MODELS = ["gpt-4o", "gpt-4o-mini-2024-07-18"]
# Default LLM model to use for answer generation
DEFAULT_LLM_MODEL = "gpt-4o" # Changed default to gpt-4o as requested

# --- Retrieval Configuration ---
DEFAULT_K = 5 # Default number of chunks to retrieve
DEFAULT_RERANK_ENABLED = True # Default state for enabling/disabling re-ranking (primarily affects GPU mode)

# --- Device Configuration ---
DEVICE = "cuda" if USE_GPU else "cpu"
print(f"Using device: {DEVICE}")

# med_discover_ai/embeddings.py
import numpy as np
import torch
from med_discover_ai.config import (
    USE_GPU, ARTICLE_ENCODER_MODEL, QUERY_ENCODER_MODEL, EMBEDDING_MODEL,
    MAX_ARTICLE_LENGTH, MAX_QUERY_LENGTH, DEVICE, OPENAI_API_KEY
)
from openai import OpenAI, APIKeyMissingError

# --- Global Variables for Models (initialized conditionally) ---
article_tokenizer = None
article_model = None
query_tokenizer = None
query_model = None
openai_client = None

# --- Initialization ---
def initialize_models():
    """Initializes models based on GPU availability."""
    global article_tokenizer, article_model, query_tokenizer, query_model, openai_client

    if USE_GPU:
        try:
            from transformers import AutoTokenizer, AutoModel
            print("Loading MedCPT models for GPU...")
            article_tokenizer = AutoTokenizer.from_pretrained(ARTICLE_ENCODER_MODEL)
            article_model = AutoModel.from_pretrained(ARTICLE_ENCODER_MODEL).to(DEVICE)
            article_model.eval() # Set model to evaluation mode

            query_tokenizer = AutoTokenizer.from_pretrained(QUERY_ENCODER_MODEL)
            query_model = AutoModel.from_pretrained(QUERY_ENCODER_MODEL).to(DEVICE)
            query_model.eval() # Set model to evaluation mode
            print("MedCPT models loaded successfully.")
        except ImportError:
            print("Error: 'transformers' library not found. Cannot use MedCPT models.")
            # Fallback or raise error? For now, just print.
        except Exception as e:
            print(f"Error loading MedCPT models: {e}")

    # Initialize OpenAI client regardless of GPU, as it's needed for LLM generation
    # and potentially for CPU-based embeddings.
    try:
        # The client automatically uses the OPENAI_API_KEY environment variable if set.
        # If not set, it might raise an error later when an API call is made,
        # unless the key is provided dynamically (e.g., via the UI).
        openai_client = OpenAI()
        # You could add a check here if needed, but typically the error occurs on the first API call.
        # try:
        #     openai_client.models.list() # Example lightweight call to check connectivity/key
        #     print("OpenAI client initialized successfully.")
        # except APIKeyMissingError:
        #     print("Warning: OpenAI API Key is missing. Please set it via environment variable or UI.")
        # except Exception as e:
        #     print(f"Warning: Could not verify OpenAI connection: {e}")
    except Exception as e:
        print(f"Error initializing OpenAI client: {e}")

# Call initialization when the module is loaded
initialize_models()

# --- Embedding Functions ---

def embed_documents(doc_chunks, batch_size=8):
    """
    Generate embeddings for document chunks using either MedCPT (GPU) or OpenAI (CPU).

    Parameters:
        doc_chunks (list): List of text chunks.
        batch_size (int): Batch size for processing (relevant for GPU).

    Returns:
        np.array: Array of embeddings. Returns empty array on failure.
    """
    if USE_GPU and article_model and article_tokenizer:
        # --- GPU Mode: Use MedCPT Article Encoder ---
        all_embeds = []
        print(f"Embedding {len(doc_chunks)} chunks using MedCPT (GPU)...")
        for i in range(0, len(doc_chunks), batch_size):
            batch = doc_chunks[i:i + batch_size]
            try:
                with torch.no_grad(): # Disable gradient calculations for inference
                    # Tokenize the batch
                    encoded = article_tokenizer(
                        batch,
                        truncation=True,
                        padding=True,
                        return_tensors="pt",
                        max_length=MAX_ARTICLE_LENGTH
                    )
                    # Move tensors to the configured device (GPU)
                    encoded = {key: val.to(DEVICE) for key, val in encoded.items()}

                    # Get model outputs
                    outputs = article_model(**encoded)
                    # Extract the [CLS] token embedding (or mean pooling if preferred)
                    batch_embeds = outputs.last_hidden_state[:, 0, :].cpu().numpy()
                    all_embeds.append(batch_embeds)
            except Exception as e:
                print(f"Error embedding batch {i//batch_size}: {e}")
                # Optionally decide how to handle errors, e.g., skip batch, return partial results

        if all_embeds:
            print("Document embedding finished.")
            return np.vstack(all_embeds)
        else:
            print("Warning: No document embeddings were generated.")
            return np.array([])
    elif not USE_GPU and openai_client and EMBEDDING_MODEL:
        # --- CPU Mode: Use OpenAI Embedding API ---
        embeddings = []
        print(f"Embedding {len(doc_chunks)} chunks using OpenAI '{EMBEDDING_MODEL}' (CPU)...")
        for i, text in enumerate(doc_chunks):
            try:
                # Ensure text is not empty or just whitespace
                if not text or text.isspace():
                    print(f"Warning: Skipping empty chunk at index {i}.")
                    # Add a zero vector or handle as appropriate
                    # For now, skipping, which might cause index misalignment if not handled later
                    continue

                response = openai_client.embeddings.create(input=text, model=EMBEDDING_MODEL)
                embed = response.data[0].embedding
                embeddings.append(embed)
            except APIKeyMissingError:
                 print("Error: OpenAI API Key is missing. Cannot generate embeddings. Please set the key.")
                 return np.array([]) # Stop embedding process
            except Exception as e:
                print(f"Error embedding chunk {i} with OpenAI: {e}")
                # Handle error, e.g., skip chunk, add zero vector
        print("Document embedding finished.")
        return np.array(embeddings)
    else:
        # Handle cases where models/clients aren't initialized
        if USE_GPU:
            print("Error: MedCPT models not available for GPU embedding.")
        else:
            print(f"Error: OpenAI client or model '{EMBEDDING_MODEL}' not available for CPU embedding.")
        return np.array([])

def embed_query(query):
    """
    Generate embedding for a single query using either MedCPT (GPU) or OpenAI (CPU).

    Parameters:
        query (str): Input query.

    Returns:
        np.array: Query embedding (shape [1, embedding_dim]). Returns None on failure.
    """
    if USE_GPU and query_model and query_tokenizer:
        # --- GPU Mode: Use MedCPT Query Encoder ---
        print("Embedding query using MedCPT (GPU)...")
        try:
            with torch.no_grad():
                encoded = query_tokenizer(
                    query,
                    truncation=True,
                    padding=True,
                    return_tensors="pt",
                    max_length=MAX_QUERY_LENGTH
                )
                encoded = {key: val.to(DEVICE) for key, val in encoded.items()}
                outputs = query_model(**encoded)
                query_embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()
            print("Query embedding finished.")
            return query_embedding # Shape should be [1, 768]
        except Exception as e:
            print(f"Error embedding query with MedCPT: {e}")
            return None
    elif not USE_GPU and openai_client and EMBEDDING_MODEL:
        # --- CPU Mode: Use OpenAI Embedding API ---
        print(f"Embedding query using OpenAI '{EMBEDDING_MODEL}' (CPU)...")
        try:
             # Ensure query is not empty
            if not query or query.isspace():
                print("Error: Cannot embed empty query.")
                return None

            response = openai_client.embeddings.create(input=query, model=EMBEDDING_MODEL)
            embed = response.data[0].embedding
            print("Query embedding finished.")
            return np.array([embed]) # Shape needs to be [1, 1536] for FAISS search
        except APIKeyMissingError:
            print("Error: OpenAI API Key is missing. Cannot generate query embedding. Please set the key.")
            return None
        except Exception as e:
            print(f"Error embedding query with OpenAI: {e}")
            return None
    else:
        # Handle cases where models/clients aren't initialized
        if USE_GPU:
            print("Error: MedCPT query model not available for GPU embedding.")
        else:
            print(f"Error: OpenAI client or model '{EMBEDDING_MODEL}' not available for CPU embedding.")
        return None

from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from rouge_score import rouge_scorer

def evaluate_response(ground_truth, response):
    """
    Evaluate an LLM response against a ground truth using ROUGE and BLEU scores.
    
    Parameters:
        ground_truth (str): The expected answer.
        response (str): The LLM-generated answer.
        
    Returns:
        dict: A dictionary with ROUGE-1, ROUGE-2, ROUGE-L, and BLEU scores.
    """
    rouge_scorer_instance = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    smoothing_function = SmoothingFunction().method4
    
    rouge_scores = rouge_scorer_instance.score(ground_truth, response)
    bleu_score = sentence_bleu([ground_truth.split()], response.split(), smoothing_function=smoothing_function)
    return {
        "rouge1": rouge_scores["rouge1"].fmeasure,
        "rouge2": rouge_scores["rouge2"].fmeasure,
        "rougeL": rouge_scores["rougeL"].fmeasure,
        "bleu": bleu_score
    }
# med_discover_ai/gradio_app.py
import gradio as gr
import os
import json
import signal
import threading
import time
import traceback # For detailed error logging

# Import necessary functions from your modules
from med_discover_ai.pdf_utils import extract_text_from_pdf
from med_discover_ai.chunking import chunk_text
from med_discover_ai.embeddings import embed_documents, initialize_models as initialize_embedding_models
from med_discover_ai.index import build_faiss_index, save_index, load_index
from med_discover_ai.retrieval import search_and_rerank, load_metadata, initialize_reranker
from med_discover_ai.llm_inference import get_llm_answer
from med_discover_ai.config import (
    INDEX_SAVE_PATH, DOC_META_PATH, OPENAI_API_KEY,
    AVAILABLE_LLM_MODELS, DEFAULT_LLM_MODEL,
    DEFAULT_K, DEFAULT_RERANK_ENABLED, USE_GPU
)

# --- Global State ---
# These hold the active index and metadata for the current session
global_index = None
global_metadata = None
# Flag to indicate if the index is ready
index_ready = False

# --- Initialization ---
def initialize_backend():
    """Initialize models and load existing index/metadata if available."""
    global global_index, global_metadata, index_ready
    print("Initializing backend...")
    initialize_embedding_models() # Load embedding models (MedCPT or prepares OpenAI client)
    initialize_reranker()       # Load re-ranker model (MedCPT cross-encoder if GPU)

    # Try to load existing index and metadata
    if os.path.exists(INDEX_SAVE_PATH) and os.path.exists(DOC_META_PATH):
        print("Attempting to load existing index and metadata...")
        loaded_index = load_index(INDEX_SAVE_PATH)
        loaded_meta = load_metadata(DOC_META_PATH)
        if loaded_index is not None and loaded_meta is not None:
            global_index = loaded_index
            global_metadata = loaded_meta
            index_ready = True
            print("Existing index and metadata loaded successfully.")
        else:
            print("Failed to load existing index or metadata. Please process PDFs.")
            index_ready = False
    else:
        print("No existing index/metadata found. Please process PDFs.")
        index_ready = False

# Call initialization when the Gradio app starts
initialize_backend()

# --- Gradio Interface Functions ---

def set_api_key(api_key):
    """Sets the OpenAI API key in the environment."""
    if not api_key or api_key == "YOUR_OPENAI_API_KEY_HERE" or api_key.isspace():
        return "Please enter a valid OpenAI API key."
    try:
        os.environ["OPENAI_API_KEY"] = api_key
        # Re-initialize models/clients that depend on the key
        initialize_embedding_models() # Re-initializes OpenAI client
        # LLM client might also need re-initialization if it failed before
        # (Assuming get_llm_answer handles client re-check)
        print("OpenAI API Key set in environment.")
        return "API key set successfully! Backend re-initialized."
    except Exception as e:
        print(f"Error setting API key: {e}")
        return f"Error setting API key: {e}"

def process_pdfs_interface(pdf_files_list, progress=gr.Progress()):
    """
    Gradio interface function to process uploaded PDFs.
    Handles text extraction, chunking, embedding, index building, and saving.

    Parameters:
        pdf_files_list (list): List of file paths provided by gr.File component.
        progress (gr.Progress): Gradio progress tracker.

    Returns:
        str: Status message indicating success or failure.
    """
    global global_index, global_metadata, index_ready
    index_ready = False # Reset status

    if not pdf_files_list:
        return "No PDF files uploaded. Please upload files to process."

    print(f"Processing {len(pdf_files_list)} PDF files...")
    all_chunks = []
    metadata = []
    doc_id_counter = 0

    # Setup progress tracking
    total_files = len(pdf_files_list)
    progress(0, desc="Starting PDF Processing...")

    try:
        for i, file_obj in enumerate(pdf_files_list):
            file_path = file_obj.name # Get the actual file path from the Gradio File object
            original_filename = os.path.basename(file_path)
            progress((i + 0.1) / total_files, desc=f"Extracting text from {original_filename}...")
            print(f"Processing file: {original_filename} ({file_path})")

            # 1. Extract Text
            text = extract_text_from_pdf(file_path)
            if not text or text.startswith("Error reading"):
                print(f"Warning: Could not extract text from {original_filename}. Skipping.")
                continue # Skip this file

            # 2. Chunk Text
            progress((i + 0.3) / total_files, desc=f"Chunking text for {original_filename}...")
            chunks = chunk_text(text, chunk_size=500, overlap=50) # Using config values ideally
            if not chunks:
                 print(f"Warning: No chunks generated for {original_filename}. Skipping.")
                 continue

            # 3. Prepare Metadata and Collect Chunks
            for chunk_id, chunk_text_content in enumerate(chunks):
                metadata.append({
                    "doc_id": doc_id_counter,
                    "filename": original_filename,
                    "chunk_id": chunk_id,
                    "text": chunk_text_content
                })
                all_chunks.append(chunk_text_content)

            doc_id_counter += 1
            print(f"Finished initial processing for {original_filename}.")

        if not all_chunks:
            return "Error: No text could be extracted or chunked from the provided PDFs."

        # 4. Embed all collected chunks
        progress(0.8, desc=f"Embedding {len(all_chunks)} text chunks...")
        print(f"Starting embedding process for {len(all_chunks)} chunks...")
        embeddings = embed_documents(all_chunks)
        if embeddings is None or embeddings.shape[0] == 0:
             return "Error: Failed to generate embeddings for the text chunks."
        if embeddings.shape[0] != len(all_chunks):
            # This indicates some chunks might have been skipped during embedding
            print(f"Warning: Number of embeddings ({embeddings.shape[0]}) does not match number of chunks ({len(all_chunks)}). Metadata might be misaligned.")
            # Adjust metadata to match embeddings - this is complex, safer to error out for now
            return "Error: Mismatch between chunks and generated embeddings. Please check logs."


        print(f"Embeddings generated successfully. Shape: {embeddings.shape}")

        # 5. Build FAISS Index
        progress(0.9, desc="Building FAISS index...")
        print("Building FAISS index...")
        index = build_faiss_index(embeddings)
        if index is None:
            return "Error: Failed to build the FAISS index."

        # 6. Save Index and Metadata & Update Global State
        progress(0.95, desc="Saving index and metadata...")
        print("Saving index and metadata...")
        index_saved = save_index(index, INDEX_SAVE_PATH)
        meta_saved = False
        try:
            # Ensure directory exists for metadata
            os.makedirs(os.path.dirname(DOC_META_PATH), exist_ok=True)
            with open(DOC_META_PATH, "w", encoding='utf-8') as f:
                json.dump(metadata, f, indent=4)
            meta_saved = True
            print("Metadata saved successfully.")
        except Exception as e:
            print(f"Error saving metadata: {e}")

        if index_saved and meta_saved:
            global_index = index
            global_metadata = metadata
            index_ready = True
            progress(1.0, desc="Processing Complete!")
            return f"Successfully processed {doc_id_counter} PDFs. Index built with {index.ntotal} vectors. Ready to chat!"
        else:
            return "Error: Index or metadata saving failed. Please check logs."

    except Exception as e:
        print(f"An unexpected error occurred during PDF processing: {e}")
        print(traceback.format_exc()) # Print detailed traceback
        index_ready = False # Ensure index is marked as not ready
        return f"An error occurred: {e}. Check console logs for details."


def query_chat_interface(query, llm_model, k_value, rerank_enabled):
    """
    Gradio interface function to handle user queries, perform retrieval,
    re-ranking (optional), and LLM generation.

    Parameters:
        query (str): The user's query text.
        llm_model (str): The selected LLM model name.
        k_value (int): The number of chunks to retrieve (k).
        rerank_enabled (bool): Whether to enable re-ranking.

    Returns:
        str: The formatted response string including the answer and context info.
    """
    global global_index, global_metadata, index_ready

    if not index_ready or global_index is None or global_metadata is None:
        return "Error: Index is not ready. Please process PDF files first."

    if not query or query.isspace():
        return "Please enter a query."

    print(f"Received query: '{query}' with LLM={llm_model}, k={k_value}, re-rank={rerank_enabled}")

    try:
        # 1. Search and Re-rank
        print("Performing search and re-rank...")
        # Ensure k_value is an integer
        try:
            k_value = int(k_value)
        except ValueError:
            print(f"Warning: Invalid k value '{k_value}', using default {DEFAULT_K}.")
            k_value = DEFAULT_K

        candidates = search_and_rerank(
            query=query,
            index=global_index,
            doc_metadata=global_metadata,
            k=k_value,
            enable_rerank=rerank_enabled
        )

        if not candidates:
            return "Could not find relevant information for your query in the processed documents."

        print(f"Retrieved {len(candidates)} candidates after processing.")
        # Display top candidate info for debugging/info
        top_cand = candidates[0]
        ret_score = top_cand.get('retrieval_score', 'N/A')
        rerank_score = top_cand.get('rerank_score', 'N/A')
        print(f"Top candidate: File='{top_cand.get('filename', 'N/A')}', Chunk={top_cand.get('chunk_id', 'N/A')}, RetScore={ret_score}, RerankScore={rerank_score}")


        # 2. Generate LLM Answer
        print("Generating LLM answer...")
        answer, context_text = get_llm_answer(query, candidates, llm_model=llm_model)

        # 3. Format Response
        response = f"**Answer:**\n{answer}\n\n---\n"
        response += f"**Context Used (from {len(candidates)} chunks, top source shown):**\n"
        response += f"Source: {top_cand.get('filename', 'N/A')} (Chunk {top_cand.get('chunk_id', 'N/A')})\n"
        # Show a snippet of the context used
        context_snippet = context_text[:300].replace('\n', ' ') # Limit length and remove newlines for display
        response += f"Snippet: {context_snippet}...\n"
        response += f"(Retrieval Score: {ret_score:.4f}"
        if 'rerank_score' in top_cand:
             response += f", Re-rank Score: {rerank_score:.4f})"
        else:
             response += ")"


        return response

    except Exception as e:
        print(f"An unexpected error occurred during query processing: {e}")
        print(traceback.format_exc()) # Print detailed traceback
        return f"An error occurred: {e}. Check console logs for details."


def shutdown_app():
    """Attempts to gracefully shut down the Gradio server."""
    print("Shutdown requested...")
    def stop():
        time.sleep(1) # Give Gradio a moment to process the request
        try:
            os.kill(os.getpid(), signal.SIGTERM) # Send termination signal
        except Exception as e:
            print(f"Error sending SIGTERM: {e}")
            # Fallback for environments where SIGTERM might not work as expected
            os._exit(1)
    # Run shutdown in a separate thread to allow the Gradio response to be sent
    threading.Thread(target=stop).start()
    return "Server shutdown initiated. You may need to close the window/tab manually."

# --- Build Gradio Interface ---
def build_interface():
    """Creates the Gradio interface layout and connects components."""
    with gr.Blocks(theme=gr.themes.Soft(), title="MedDiscover") as demo:
        gr.Markdown("# ðŸ©º MedDiscover: Biomedical Research Assistant")
        gr.Markdown("Upload research papers (PDF), ask questions, and get answers powered by RAG and LLMs.")

        with gr.Tabs():
            # --- Setup Tab ---
            with gr.TabItem("Setup & PDF Processing"):
                gr.Markdown("### 1. Configure OpenAI API Key")
                with gr.Row():
                    api_key_input = gr.Textbox(
                        label="OpenAI API Key",
                        type="password",
                        placeholder="Enter your sk-... key here",
                        value=OPENAI_API_KEY if OPENAI_API_KEY != "YOUR_OPENAI_API_KEY_HERE" else "",
                        scale=3
                    )
                    api_key_button = gr.Button("Set API Key", scale=1)
                api_key_status = gr.Textbox(label="API Key Status", interactive=False)

                gr.Markdown("### 2. Upload and Process PDFs")
                gr.Markdown("Upload the PDF documents you want to query. Processing involves text extraction, chunking, embedding generation, and index building. This may take time depending on the number and size of PDFs and your hardware (GPU significantly speeds up embedding).")
                # Changed type to 'filepath' as it seemed required by the original process_pdfs logic
                pdf_input = gr.File(
                    label="Upload PDF Files",
                    file_count="multiple",
                    file_types=[".pdf"],
                    type="filepath" # Passes list of file paths
                )
                process_button = gr.Button("Process Uploaded PDFs", variant="primary")
                process_output = gr.Textbox(label="Processing Status", interactive=False, lines=3)

                gr.Markdown("### 3. Server Control")
                shutdown_button = gr.Button("Shutdown Server")
                shutdown_output = gr.Textbox(label="Server Status", interactive=False)


            # --- Chat Tab ---
            with gr.TabItem("Chat & Query"):
                gr.Markdown("### Ask Questions About Your Documents")
                gr.Markdown("Ensure PDFs have been processed successfully before asking questions.")

                with gr.Row():
                    with gr.Column(scale=3):
                         query_input = gr.Textbox(label="Enter your query here", lines=3, placeholder="e.g., What biomarkers are associated with Gaucher Disease?")
                         chat_button = gr.Button("Get Answer", variant="primary")
                    with gr.Column(scale=1):
                        gr.Markdown("#### Query Options")
                        llm_model_dropdown = gr.Dropdown(
                            label="LLM Model",
                            choices=AVAILABLE_LLM_MODELS,
                            value=DEFAULT_LLM_MODEL, # Use the default from config
                            info="Select the OpenAI model for answer generation."
                        )
                        # Advanced Settings Accordion
                        with gr.Accordion("Advanced Retrieval Settings", open=False):
                             k_slider = gr.Slider(
                                 label="Number of Chunks (k)",
                                 minimum=1,
                                 maximum=20,
                                 step=1,
                                 value=DEFAULT_K, # Use default from config
                                 info="How many text chunks to retrieve initially."
                             )
                             rerank_checkbox = gr.Checkbox(
                                 label="Enable Re-ranking (GPU Only)",
                                 value=DEFAULT_RERANK_ENABLED, # Use default from config
                                 info="Use MedCPT Cross-Encoder for relevance re-ranking (requires GPU)."
                             )


                chat_output = gr.Textbox(label="Response", lines=10, interactive=False)


        # --- Connect Components ---
        # Setup Tab Connections
        api_key_button.click(
            fn=set_api_key,
            inputs=api_key_input,
            outputs=api_key_status
        )
        process_button.click(
            fn=process_pdfs_interface,
            inputs=pdf_input,
            outputs=process_output
        )
        shutdown_button.click(
            fn=shutdown_app,
            inputs=None,
            outputs=shutdown_output,
            api_name="shutdown" # Optional: Define API name for programmatic access
        )

        # Chat Tab Connections
        chat_button.click(
            fn=query_chat_interface,
            inputs=[query_input, llm_model_dropdown, k_slider, rerank_checkbox],
            outputs=chat_output,
            api_name="query" # Optional: Define API name
        )
        # Allow submitting query with Enter key
        query_input.submit(
             fn=query_chat_interface,
            inputs=[query_input, llm_model_dropdown, k_slider, rerank_checkbox],
            outputs=chat_output
        )


    return demo

# --- Main Execution ---
if __name__ == "__main__":
    # Build the Gradio interface
    med_discover_app = build_interface()

    # Launch the application
    # share=True creates a public link (use with caution)
    # server_name="0.0.0.0" makes it accessible on the local network
    print("Launching MedDiscover Gradio App...")
    med_discover_app.launch(server_name="0.0.0.0", server_port=7860)
    # Add share=True if you need a public link: med_discover_app.launch(share=True)

# med_discover_ai/index.py
import faiss
import numpy as np
import os
from med_discover_ai.config import INDEX_SAVE_PATH, EMBEDDING_DIMENSION, USE_GPU

def build_faiss_index(embeddings):
    """
    Build a FAISS index from the given embeddings.

    Parameters:
        embeddings (np.array): Array of embeddings (shape [N, D]).

    Returns:
        faiss.Index or None: FAISS index if successful, None otherwise.
    """
    if embeddings is None or embeddings.shape[0] == 0:
        print("Error: Cannot build index from empty or invalid embeddings.")
        return None

    dimension = embeddings.shape[1]

    # Verify dimension consistency
    expected_dim = EMBEDDING_DIMENSION
    if dimension != expected_dim:
        print(f"Warning: Embedding dimension mismatch. Expected {expected_dim}, got {dimension}.")
        # Decide how to handle: error out, or proceed with caution?
        # For now, proceed but log warning. If critical, raise an error.

    print(f"Building FAISS index with dimension {dimension}...")

    try:
        # Using Inner Product (IP) index for MedCPT as recommended by original paper/examples.
        # For OpenAI embeddings (like ada-002), L2 (Euclidean distance) is more common.
        # We choose based on the mode.
        if USE_GPU:
            # MedCPT typically uses Inner Product (cosine similarity after normalization)
            print("Using IndexFlatIP (Inner Product) for MedCPT embeddings.")
            index = faiss.IndexFlatIP(dimension)
        else:
            # OpenAI embeddings often use L2 (Euclidean distance)
            print("Using IndexFlatL2 (Euclidean Distance) for OpenAI embeddings.")
            index = faiss.IndexFlatL2(dimension)

        # FAISS requires float32 type
        if embeddings.dtype != np.float32:
            print("Converting embeddings to float32 for FAISS.")
            embeddings = embeddings.astype(np.float32)

        # Add embeddings to the index
        index.add(embeddings)
        print(f"FAISS index built successfully with {index.ntotal} vectors.")
        return index
    except Exception as e:
        print(f"Error building FAISS index: {e}")
        return None

def save_index(index, path=INDEX_SAVE_PATH):
    """
    Save the FAISS index to disk.

    Parameters:
        index (faiss.Index): The FAISS index to save.
        path (str): The file path to save the index to.

    Returns:
        bool: True if saving was successful, False otherwise.
    """
    if index is None:
        print("Error: Cannot save a null index.")
        return False
    try:
        print(f"Saving FAISS index to {path}...")
        # Ensure directory exists
        os.makedirs(os.path.dirname(path), exist_ok=True)
        faiss.write_index(index, path)
        print("Index saved successfully.")
        return True
    except Exception as e:
        print(f"Error saving FAISS index to {path}: {e}")
        return False

def load_index(path=INDEX_SAVE_PATH):
    """
    Load the FAISS index from disk.

    Parameters:
        path (str): The file path to load the index from.

    Returns:
        faiss.Index or None: The loaded FAISS index, or None if loading fails.
    """
    if not os.path.exists(path):
        print(f"Error: Index file not found at {path}.")
        return None
    try:
        print(f"Loading FAISS index from {path}...")
        index = faiss.read_index(path)
        print(f"Index loaded successfully with {index.ntotal} vectors and dimension {index.d}.")
        # Optional: Verify dimension against config
        if index.d != EMBEDDING_DIMENSION:
             print(f"Warning: Loaded index dimension ({index.d}) differs from config ({EMBEDDING_DIMENSION}).")
        return index
    except Exception as e:
        print(f"Error loading FAISS index from {path}: {e}")
        return None

# med_discover_ai/llm_inference.py
import openai
from openai import OpenAI, APIKeyMissingError
from med_discover_ai.config import DEFAULT_LLM_MODEL # Import default

# --- Global OpenAI Client ---
# Reuse the client initialized in embeddings.py if possible, or initialize here.
# For simplicity here, we assume it might need its own initialization or rely on the global env var.
try:
    client = OpenAI()
    # Check if the API key is available
    # client.models.list() # Optional check - might fail if key is missing
except APIKeyMissingError:
    print("LLM Inference Warning: OpenAI API Key is missing. Set it via environment or UI.")
    client = None # Indicate client is not ready
except Exception as e:
    print(f"LLM Inference Error: Could not initialize OpenAI client: {e}")
    client = None

# --- LLM Answer Generation ---
def get_llm_answer(query, retrieved_candidates, llm_model=DEFAULT_LLM_MODEL):
    """
    Generate an answer using a specified OpenAI LLM based on retrieved candidate texts.

    Parameters:
        query (str): The user's original query.
        retrieved_candidates (list): A list of candidate dictionaries, sorted by relevance.
                                     Each dictionary should have a "text" key.
        llm_model (str): The specific OpenAI model to use (e.g., "gpt-4o", "gpt-4o-mini-2024-07-18").

    Returns:
        tuple: (str, str) containing:
               - The generated answer string, or an error message.
               - The context string used for generation, or an empty string if no context.
    """
    global client # Access the potentially initialized client

    if client is None:
        # Try to re-initialize if it wasn't ready before (e.g., key set later via UI)
        try:
            client = OpenAI()
            # client.models.list() # Optional check
        except APIKeyMissingError:
             return "Error: OpenAI API Key is missing. Cannot generate answer.", ""
        except Exception as e:
             return f"Error: Failed to initialize OpenAI client: {e}", ""

    if not retrieved_candidates:
        print("Warning: No candidates provided to generate LLM answer.")
        # Decide how to handle: Query LLM without context, or return specific message?
        # For now, let's try querying without context, but adjust prompt.
        context_text = ""
        prompt = f"""
        Answer the following question concisely, in as few words as possible, based on general knowledge.

        Question: {query}

        Answer (in minimal words):
        """
    else:
        # Combine the top candidate texts into a context.
        # Consider limiting the context size if necessary, e.g., join only top 3 candidates
        context_text = " ".join([cand["text"] for cand in retrieved_candidates]) # Using all provided candidates
        # Optional: Add truncation logic here if context_text is too long for the model

        prompt = f"""
        You are Med-Discover, an assistant for enhancing disease discovery, using provided context from research papers.
        Use ONLY the context below to answer the question in as few words as possible. If the context doesn't contain the answer, say "Information not found in context".

        Context:
        {context_text}
        ---
        Question: {query}

        Answer (in minimal words, based ONLY on context):
        """

    print(f"Generating LLM answer using model: {llm_model}...")

    try:
        # Use the ChatCompletion endpoint
        response = client.chat.completions.create(
            model=llm_model,
            messages=[
                # System message sets the persona (optional but good practice)
                # {"role": "system", "content": "You are Med-Discover, an AI assistant specialized in biomedical research."},
                # User message contains the prompt with context and question
                {"role": "user", "content": prompt}
            ],
            max_tokens=50,  # Increased slightly for potentially more nuanced short answers
            temperature=0.1 # Low temperature for factual, concise answers
            # top_p=1.0, # Default
            # frequency_penalty=0.0, # Default
            # presence_penalty=0.0 # Default
        )

        # Extract the answer from the response
        answer = response.choices[0].message.content.strip()
        print("LLM answer generated successfully.")
        return answer, context_text

    except APIKeyMissingError:
        print("Error: OpenAI API Key is missing. Cannot generate LLM answer.")
        return "Error: OpenAI API Key is missing.", context_text
    except openai.RateLimitError:
        print("Error: OpenAI rate limit exceeded.")
        return "Error: Rate limit exceeded. Please try again later.", context_text
    except openai.AuthenticationError:
         print("Error: OpenAI authentication failed. Check your API key.")
         return "Error: Invalid OpenAI API Key.", context_text
    except Exception as e:
        print(f"Error during LLM inference with model {llm_model}: {e}")
        return f"Error generating answer: {e}", context_text

from med_discover_ai.gradio_app import build_interface

def main():
    demo = build_interface()
    demo.launch(share=True)

if __name__ == "__main__":
    main()import os
import PyPDF2

def extract_text_from_pdf(pdf_path):
    """
    Extract text from a PDF file.
    
    Parameters:
        pdf_path (str): Path to the PDF file.
        
    Returns:
        str: Extracted text.
    """
    text = ""
    try:
        with open(pdf_path, "rb") as f:
            reader = PyPDF2.PdfReader(f)
            for page in reader.pages:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n"
    except Exception as e:
        text = f"Error reading {pdf_path}: {str(e)}"

    return text


def process_pdf_folder(pdf_folder):
    """
    Process all PDF files in a folder.
    
    Parameters:
        pdf_folder (str): Path to the folder containing PDFs.
        
    Returns:
        list: List of dictionaries with filename and extracted text.
    """
    pdf_data = []
    for filename in os.listdir(pdf_folder):
        if filename.lower().endswith(".pdf"):
            path = os.path.join(pdf_folder, filename)
            text = extract_text_from_pdf(path)
            pdf_data.append({"filename": filename, "text": text})
    return pdf_data
# med_discover_ai/retrieval.py
import torch
import numpy as np
import json
import os
from med_discover_ai.config import (
    USE_GPU, CROSS_ENCODER_MODEL, MAX_ARTICLE_LENGTH, DOC_META_PATH, DEVICE,
    DEFAULT_K, DEFAULT_RERANK_ENABLED
)
from med_discover_ai.embeddings import embed_query
# Removed index loading from here, should be passed in

# --- Global Variables for Re-ranking Model (initialized conditionally) ---
cross_tokenizer = None
cross_model = None

# --- Initialization ---
def initialize_reranker():
    """Initializes the re-ranking model if GPU is available."""
    global cross_tokenizer, cross_model
    if USE_GPU:
        try:
            from transformers import AutoTokenizer, AutoModelForSequenceClassification
            print("Loading MedCPT Cross-Encoder model for re-ranking (GPU)...")
            cross_tokenizer = AutoTokenizer.from_pretrained(CROSS_ENCODER_MODEL)
            cross_model = AutoModelForSequenceClassification.from_pretrained(CROSS_ENCODER_MODEL).to(DEVICE)
            cross_model.eval() # Set model to evaluation mode
            print("Cross-Encoder model loaded successfully.")
        except ImportError:
            print("Error: 'transformers' library not found. Cannot use MedCPT Cross-Encoder.")
        except Exception as e:
            print(f"Error loading MedCPT Cross-Encoder model: {e}")
    else:
        print("Re-ranking with Cross-Encoder is disabled (CPU mode).")

# Call initialization when the module is loaded
initialize_reranker()

# --- Metadata Loading ---
def load_metadata(meta_path=DOC_META_PATH):
    """
    Load document metadata from a JSON file.

    Parameters:
        meta_path (str): Path to the metadata JSON file.

    Returns:
        list or None: List of document metadata dictionaries, or None if loading fails.
    """
    if not os.path.exists(meta_path):
        print(f"Error: Metadata file not found at {meta_path}.")
        return None
    try:
        with open(meta_path, "r", encoding='utf-8') as f:
            metadata = json.load(f)
        print(f"Metadata loaded successfully from {meta_path}.")
        return metadata
    except json.JSONDecodeError as e:
        print(f"Error decoding JSON from {meta_path}: {e}")
        return None
    except Exception as e:
        print(f"Error loading metadata from {meta_path}: {e}")
        return None

# --- Re-ranking Function ---
def rerank_candidates(query, candidates):
    """
    Re-ranks candidate documents using the MedCPT Cross-Encoder.
    Requires GPU and initialized cross-encoder model.

    Parameters:
        query (str): The user query.
        candidates (list): List of candidate dictionaries, each must have a "text" key.

    Returns:
        np.array or None: An array of relevance scores (logits) for each candidate,
                          or None if re-ranking cannot be performed.
    """
    if not USE_GPU or not cross_model or not cross_tokenizer:
        print("Re-ranking skipped: GPU not available or Cross-Encoder model not loaded.")
        return None # Indicate that re-ranking was not performed

    if not candidates:
        print("Warning: No candidates provided for re-ranking.")
        return np.array([])

    print(f"Re-ranking {len(candidates)} candidates using MedCPT Cross-Encoder (GPU)...")
    # Prepare pairs for the cross-encoder: [query, candidate_text]
    pairs = [[query, candidate["text"]] for candidate in candidates]

    try:
        with torch.no_grad(): # Disable gradient calculations for inference
            # Tokenize the pairs
            encoded = cross_tokenizer(
                pairs,
                truncation=True,
                padding=True,
                return_tensors="pt",
                max_length=MAX_ARTICLE_LENGTH # Use appropriate max length for cross-encoder
            )
            # Move tensors to the configured device (GPU)
            encoded = {key: val.to(DEVICE) for key, val in encoded.items()}

            # Get model outputs (logits represent relevance score)
            outputs = cross_model(**encoded)
            logits = outputs.logits.squeeze(dim=1) # Remove unnecessary dimension

        print("Re-ranking finished.")
        return logits.cpu().numpy() # Return scores as a NumPy array
    except Exception as e:
        print(f"Error during re-ranking with Cross-Encoder: {e}")
        return None # Indicate failure

# --- Combined Search and Re-ranking ---
def search_and_rerank(query, index, doc_metadata, k=DEFAULT_K, enable_rerank=DEFAULT_RERANK_ENABLED):
    """
    Performs dense retrieval using FAISS, optionally re-ranks using MedCPT Cross-Encoder (GPU only),
    and returns sorted candidate documents.

    Parameters:
        query (str): The user query.
        index (faiss.Index): The loaded FAISS index.
        doc_metadata (list): List of document metadata dictionaries.
        k (int): Number of top results to retrieve initially.
        enable_rerank (bool): Whether to perform re-ranking (only applies if USE_GPU is True).

    Returns:
        list: A list of candidate document dictionaries, sorted by relevance.
              Each dictionary includes 'text', 'filename', 'chunk_id', 'retrieval_score',
              and potentially 'rerank_score'. Returns empty list on major failure.
    """
    if not query or query.isspace():
        print("Error: Cannot search with an empty query.")
        return []
    if index is None:
        print("Error: FAISS index is not available.")
        return []
    if doc_metadata is None:
        print("Error: Document metadata is not available.")
        return []

    # Step 1: Embed the query
    print(f"Embedding query for search (k={k}, re-rank={enable_rerank})...")
    query_embedding = embed_query(query)
    if query_embedding is None:
        print("Error: Failed to embed query. Aborting search.")
        return []

    # Ensure query embedding is float32 for FAISS
    if query_embedding.dtype != np.float32:
        query_embedding = query_embedding.astype(np.float32)

    # Step 2: Dense Retrieval using FAISS
    print(f"Performing FAISS search for top {k} candidates...")
    try:
        # `index.search` returns distances/scores and indices
        # For IndexFlatL2 (CPU/OpenAI): scores are squared Euclidean distances (lower is better)
        # For IndexFlatIP (GPU/MedCPT): scores are inner products (higher is better)
        scores, inds = index.search(query_embedding, k)
        print(f"FAISS search returned {len(inds[0])} results.")
    except Exception as e:
        print(f"Error during FAISS search: {e}")
        return []

    # Step 3: Retrieve Candidate Metadata
    candidates = []
    retrieved_indices = inds[0]
    retrieved_scores = scores[0]

    for score, ind in zip(retrieved_scores, retrieved_indices):
        if ind < 0 or ind >= len(doc_metadata):
            print(f"Warning: Invalid index {ind} returned by FAISS search. Skipping.")
            continue # Skip invalid indices

        # Create a copy to avoid modifying the original metadata list
        entry = doc_metadata[ind].copy()
        entry["retrieval_score"] = float(score) # Store the raw score from FAISS
        candidates.append(entry)

    if not candidates:
        print("No valid candidates found after FAISS search.")
        return []

    print(f"Retrieved {len(candidates)} initial candidates.")

    # Step 4: Optional Re-ranking (GPU only)
    rerank_scores = None
    if USE_GPU and enable_rerank:
        rerank_scores = rerank_candidates(query, candidates) # This returns None if it fails or is skipped
        if rerank_scores is not None:
             print("Assigning re-rank scores...")
             if len(rerank_scores) == len(candidates):
                 for i, score in enumerate(rerank_scores):
                     candidates[i]["rerank_score"] = float(score)
             else:
                 print(f"Warning: Mismatch between number of candidates ({len(candidates)}) and re-rank scores ({len(rerank_scores)}). Skipping score assignment.")
                 rerank_scores = None # Treat as if re-ranking didn't happen for sorting
        else:
            print("Re-ranking was skipped or failed.")

    # Step 5: Sort Results
    # Determine the sorting key based on whether re-ranking was performed successfully
    sort_key = "rerank_score" if (USE_GPU and enable_rerank and rerank_scores is not None) else "retrieval_score"
    # Determine reverse sort order: True for scores where higher is better (IP, rerank), False for L2 distance
    reverse_sort = True if sort_key == "rerank_score" or (sort_key == "retrieval_score" and USE_GPU) else False

    print(f"Sorting candidates by '{sort_key}' (reverse={reverse_sort})...")
    try:
        # Handle potential missing 'rerank_score' key if reranking failed midway
        candidates_sorted = sorted(
            candidates,
            key=lambda x: x.get(sort_key, -np.inf if reverse_sort else np.inf), # Default to worst score if key missing
            reverse=reverse_sort
        )
        print("Candidates sorted successfully.")
        return candidates_sorted
    except Exception as e:
        print(f"Error sorting candidates: {e}")
        return candidates # Return unsorted candidates as fallback

import pandas as pd
from scipy.stats import ks_2samp, shapiro, mannwhitneyu
import json

def perform_ks_test(data, model1, model2, metric):
    group1 = data[data["Model"] == model1][metric].dropna()
    group2 = data[data["Model"] == model2][metric].dropna()
    ks_stat, ks_p = ks_2samp(group1, group2)
    return ks_stat, ks_p

def perform_shapiro_test(data, model, metric):
    group = data[data["Model"] == model][metric].dropna()
    if len(group) >= 3:
        stat, p = shapiro(group)
        return stat, p
    else:
        return None, None

def perform_mannwhitney_test(data, model1, model2, metric):
    group1 = data[data["Model"] == model1][metric].dropna()
    group2 = data[data["Model"] == model2][metric].dropna()
    stat, p = mannwhitneyu(group1, group2)
    return stat, p

def save_statistical_results(results, filename="statistical_analysis_results.json"):
    with open(filename, "w") as f:
        json.dump(results, f, indent=4)
